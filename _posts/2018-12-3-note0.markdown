---
layout:     post
title:      "吴恩达deepAI深度学习第一周课程学习笔记"
subtitle:   " \"神经网络入门，关于池化，步长等基础操作\""
date:       2018-12-3 17:00:00
author:     "neo"
header-img: "img/post-bg-road.jpg"
catalog: true
tags:
    - CNN
    - 学习笔记
    - 机器学习
---

## <center>吴恩达deepAI深度学习第一周课程学习记录

#### 一.loss function & cost function 

loss function：一个样本的损失

cost function：将所有样本的值加在一起

在**logistic Regression**中一般直接使用带log 函数的，不使用均方误差

均方误差容易使得结果变得不收敛(convent function)。

![](http://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/007bgNxTly1g16rutewpbj316n05jmxy.jpg%29)

#### 二.导数与梯度下降

slope （斜率）= height / width  =  **derivatives**

#### 三.计算图

对应tensorflow中的graph的概念

应用chain rule 链式法则进行求导。

#### 四. 二分算法Back propagation

https://www.jianshu.com/p/498f7bf488a2

1. 基础知识

![](http://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/007bgNxTly1g16ptu86yoj315q0c2gm9.jpg%29)

![](http://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/007bgNxTly1g16pu8c8jxj30y605t3yt.jpg%29)

2. 利用chain rules 进行一步步求导
3. 逻辑回归中的梯度下降法

对单个样本而言，逻辑回归Loss function表达式：
$$
z=w^Tx+b
$$

$$
y=a=σ(z)
$$

$$
L(a,y)=−(ylog(a)+(1−y)log(1−a))
$$

反向传播过程：


前面过程的da、dz求导：

![](http://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/007bgNxTly1g16qat0iblj31c40r1myu.jpg%29)

#### 五.激活函数

不要用sigmoid（），除了对与**output layer**

**tanh(z)** 比sigmod(z) 函数好

relu() 二分输出比较好

leaky Relu（）一般比较好，但是不常用

使用激活函数的原因是因为：这样才会变成非线性。线性的函数意义不大。



#### 六.向量化

对m个样本来说，其Cost function表达式如下：

![](http://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/007bgNxTly1g16qcju7bej319u076dge.jpg%29)

Cost function 关于w和b的偏导数可以写成所有样本点偏导数和的平均形式：

![](http://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/007bgNxTly1g16qcv3rlgj311o07at8y.jpg%29)

#### 七.神经网络的梯度下降法

 以本节中的浅层神经网络为例，我们给出神经网络的梯度下降法的公式。

![](http://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/007bgNxTly1g16qg4rywjj31700j6ae2.jpg%29)

#### 八.随机化参数

注意：w不能随机化为0，这样会让神经网络每一次层中对称的神经元变为一样的。

原因：由于两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。

```python
W = np.random.rand((2,2))* 0.01
b = np.zero((2,1))
```

**给其乘以0.01是为了在使用sigmod函数时方便训练**，在0处的导数大，训练速度快。

tanh可以不乘。

