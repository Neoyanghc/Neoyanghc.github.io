---
layout:     post
title:      "论文笔记12: CRelu_深度学习模型压缩"
subtitle:   " \"最近做的可解释性文章，很多都是用在模型压缩上，因此找一些有关\""
date:       2019-10-22 13:00:00
author:     "jack"
header-img: "img/post-bg-sea.jpg"
catalog: true
tags:
    - 论文笔记
    - 机器学习
---

## 论文笔记12: CRelu_深度学习模型压缩

> Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

### 1.Introduction

论文作者在 AlexNet 的模型上做了一个有趣的实验，发现：低层的卷积层中的一些滤波器核存在着负相关程度很高的滤波器核，而层次越高的卷积层，这一现象越不明显。作者把这一现象称为 **pairing phenomenon。**

![](https://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/20191029120631.png)

然后作者利用余弦相似度对每个卷积核进行两两对pair操作，根据值进行统计。



![](https://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/20191029120529.png)

现象：网络的前部，参数的分布有更强的负相关性(类似于正负对立)。随着网络变深，这种负相关性逐步减弱。
结论：网络的前部，网络倾向于同时捕获正负相位的信息，但ReLU会抹掉负响应。 这造成了卷积核会存在冗余。

### 2. Methods

根据这一个现象,作者做出假设

>we hypothesize that despite ReLU erasing negative linear responses, the first few convolution layers of a deep CNN manage to capture both negative and positive phase information through learning pairs or groups of negatively correlated filters. This conjecture implies that there exists a redundancy among the filters

为了消除 ReLU 带来的冗余，论文作者提出了一个新的激活机制（activation scheme），称为Concatenated Rectified Linear Units，简称为CReLU:

$$CReLU(x)=Concat(Relu[x],Relu[−x])$$

举个例子，$$x=[-1,3],CRelu(x)=[[0,3],[1,0]]$$

在网络中的实现也很简单，甚至不用修改代码（通过scale层取反再经过一次ReLU）

![](https://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/20191029121510.png)

### 3.Experiments

最后作者在三个数据集上验证了CReLU的有效性：CIFAR-10，CIFAR-100和ImageNet。此外论文作者还从正则化和重建角度对CReLU的有效性进行了定性的讨论

![](https://jackyanghc-picture.oss-cn-beijing.aliyuncs.com/20191029122417.png)

